# -*- coding: utf-8 -*-
"""standford-nli.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Aayush050502/6d127695a0f392008540c730d43907bf/standford-nli.ipynb
"""

!pip install transformers datasets streamlit pyngro

from google.colab import drive
import zipfile
import os

# Mount Google Drive
drive.mount('/content/drive')

# Define paths
zip_path = '/content/drive/MyDrive/standford nli/standford natural language inference.zip'
extract_path = '/content/drive/MyDrive/standford nli'

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# Verify the extraction
os.listdir(extract_path)

import pandas as pd

# Load datasets
train_file = '/content/drive/MyDrive/standford nli/snli_1.0_train.csv'
valid_file = '/content/drive/MyDrive/standford nli/snli_1.0_dev.csv'
test_file = '/content/drive/MyDrive/standford nli/snli_1.0_test.csv'

# Load the file as a single column
df_train = pd.read_csv('/content/drive/MyDrive/standford nli/snli_1.0_train.csv', delimiter='\t', header=None)
print(df_train.head())
print(df_train.shape)

# Print the first few lines of the file to understand its structure
with open('/content/drive/MyDrive/standford nli/snli_1.0_train.csv', 'r') as file:
    for _ in range(5):
        print(file.readline())

# Load the data directly with correct delimiters and column names
df_train = pd.read_csv('/content/drive/MyDrive/standford nli/snli_1.0_train.csv', delimiter='\t', names=['sentence1', 'sentence2', 'label'])
df_valid = pd.read_csv('/content/drive/MyDrive/standford nli/snli_1.0_dev.csv', delimiter='\t', names=['sentence1', 'sentence2', 'label'])
df_test = pd.read_csv('/content/drive/MyDrive/standford nli/snli_1.0_test.csv', delimiter='\t', names=['sentence1', 'sentence2', 'label'])

# Inspect the first few rows to check the data structure
print(df_train.head())

!pip install datasets

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')

# Sample data - replace this with your actual data
data = {'sentence1': ['This is a sentence.', 'Another sentence here.'],
        'sentence2': ['This is a second sentence.', 'And a second sentence here.'],
        'label': [0, 1]}

# Import the Pandas library
import pandas as pd

# Create Pandas DataFrames
df_train = pd.DataFrame(data)
df_valid = pd.DataFrame(data)
df_test = pd.DataFrame(data)

# Convert to Hugging Face Dataset
train_dataset = Dataset.from_pandas(df_train[['sentence1', 'sentence2', 'label']])
valid_dataset = Dataset.from_pandas(df_valid[['sentence1', 'sentence2', 'label']])
test_dataset = Dataset.from_pandas(df_test[['sentence1', 'sentence2', 'label']])

train_dataset = train_dataset.map(tokenize_function, batched=True)
valid_dataset = valid_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir='/content/results',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_dir='/content/logs',
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
)

# Train the model
trainer.train()

"""IMPLEMENT REACT IN HUMAN IN LOOP

"""

class REACTAgent:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_response(self, sentence1, sentence2):
        inputs = self.tokenizer(sentence1, sentence2, return_tensors='pt', truncation=True, padding='max_length')
        outputs = self.model(**inputs)
        logits = outputs.logits
        predicted_class = logits.argmax(dim=-1).item()
        return predicted_class

# Instantiate REACT Agent
agent = REACTAgent(model, tokenizer)

# Define interaction function
def interact_with_agent(sentence1, sentence2):
    label = agent.generate_response(sentence1, sentence2)
    labels = ['Entailment', 'Contradiction', 'Neutral']
    return labels[label]

"""DEPLOY WITH STREAMLIT

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py
# import streamlit as st
# import pandas as pd
# # Load model and tokenizer
# import torch
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# 
# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
# model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
# 
# class REACTAgent:
#     def __init__(self, model, tokenizer):
#         self.model = model
#         self.tokenizer = tokenizer
# 
#     def generate_response(self, sentence1, sentence2):
#         inputs = self.tokenizer(sentence1, sentence2, return_tensors='pt', truncation=True, padding='max_length')
#         with torch.no_grad():
#             outputs = self.model(**inputs)
#         logits = outputs.logits
#         predicted_class = logits.argmax(dim=-1).item()
#         return predicted_class
# 
# agent = REACTAgent(model, tokenizer)
# 
# st.title('Human-in-the-Loop Agent for NLI')
# 
# sentence1 = st.text_input("Enter the first sentence:")
# sentence2 = st.text_input("Enter the second sentence:")
# 
# if sentence1 and sentence2:
#     label = agent.generate_response(sentence1, sentence2)
#     labels = ['Entailment', 'Contradiction', 'Neutral']
#     st.write(f"Predicted label: {labels[label]}")

!pip install streamlit
import streamlit as st

"""RUNNING STREAMLIT HERE

"""

!pip install pyngrok
from pyngrok import ngrok

# Run Streamlit app
!streamlit run streamlit_app.py &

from pyngrok import ngrok

# Set your authtoken
ngrok.set_auth_token("YOUR_NGROK_AUTH_TOKEN")

# Create a tunnel to the Streamlit server
public_url = ngrok.connect(port='8501')
print('Streamlit app running at:',Â public_url)